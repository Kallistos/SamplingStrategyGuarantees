{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore') \n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Specific warning suppressions for common issues\n",
    "warnings.filterwarnings('ignore', category=SyntaxWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=PendingDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the helper functions are defined here\n",
    "def get_n_random_samples(measurement_df, n):\n",
    "    \"\"\"\n",
    "    Draw n random samples without repetition from the measurement (pandas) DataFrame \n",
    "    \"\"\"\n",
    "    return measurement_df.sample(n, replace = False)\n",
    "\n",
    "\n",
    "def SRS_sampling(step_sample_size, measurement_df, nfp, samples_df=pd.DataFrame(), decision_set = [], sample_set_size = 300):\n",
    "    \"\"\"\n",
    "    Re-implementation of the Statistical Recursive Searching (SRS) algorithm to sample the configuration space of a software system\n",
    "    (with extreme values in mind)\n",
    "    This algorithm was originally proposed by Jeho OH. et al. at ESEC/FSE 2017\n",
    "    \"\"\"\n",
    "\n",
    "    # First filter measurements for the current decision set (exclude any configuration that does not match the decisions in the decision set)\n",
    "    filtered_df = measurement_df\n",
    "    for feature, decision in decision_set:\n",
    "        filtered_df = filtered_df[filtered_df[feature] == decision]\n",
    "\n",
    "    # Check if there are still samples missing to reach the sample set size\n",
    "    if not samples_df.empty and samples_df.shape[0] + step_sample_size >= sample_set_size:\n",
    "        step_sample_size = sample_set_size - len(samples_df.index)\n",
    "    # Draw n random samples without repetition from the filtered measurement DataFrame for the current recursive step\n",
    "    new_samples_df = get_n_random_samples(filtered_df, step_sample_size) if len(filtered_df.index) > step_sample_size else filtered_df\n",
    "    # Concatenate the new samples to the existing samples\n",
    "    samples_df = pd.concat([samples_df, new_samples_df])\n",
    "\n",
    "    # Sort the samples by the NFP of interest (ascending order if you are looking for best case performance, descending order if you are looking for worst case performance)\n",
    "    sorted_new_samples_df = new_samples_df.sort_values(by = nfp, ascending = False)\n",
    "\n",
    "    # Check if there are enough samples to compare at least 2 of them\n",
    "    if len(sorted_new_samples_df.index) < 2:\n",
    "        # If there are not enough samples to continue SRS, then sample the remaining configuration space for the missing samples\n",
    "        if len(samples_df.index) < sample_set_size:\n",
    "            sample_size = sample_set_size - len(samples_df.index)\n",
    "            new_samples_df = get_n_random_samples(measurement_df, sample_size) if len(measurement_df.index) > sample_size else measurement_df\n",
    "            samples_df = pd.concat([samples_df, new_samples_df])\n",
    "        return samples_df\n",
    "\n",
    "    # Compare the first two samples to find common decisions\n",
    "    compared_first_two_samples = sorted_new_samples_df.diff().iloc[1]\n",
    "\n",
    "    common_decisions = [] \n",
    "    for index, value in compared_first_two_samples.items():\n",
    "        # Skip the NFP column (as it is not a feature decision)\n",
    "        if index in nfp:\n",
    "            continue\n",
    "        # If the difference is 0, then the decision is common\n",
    "        if value == 0:\n",
    "            common_decisions.append(index)\n",
    "\n",
    "    noteworthy_decisions = []\n",
    "    # For each common decision, check if the performance is significantly different when the decision is done in one way or the other\n",
    "    for candidate_decision in common_decisions:\n",
    "        # Calculate the average performance for the samples where the candidate decision is enabled and disabled\n",
    "        avg_performance_enabled = sorted_new_samples_df[sorted_new_samples_df[candidate_decision] == sorted_new_samples_df.iloc[0][candidate_decision]][nfp].mean()\n",
    "        avg_performance_disabled = sorted_new_samples_df[sorted_new_samples_df[candidate_decision] != sorted_new_samples_df.iloc[0][candidate_decision]][nfp].mean()\n",
    "\n",
    "        # Check of the averages are not NaN\n",
    "        if avg_performance_enabled is np.nan or avg_performance_disabled is np.nan:\n",
    "            continue\n",
    "        # If the averages are different and not already in the decision set, then check if the difference is statistically significant\n",
    "        if avg_performance_enabled != avg_performance_disabled and candidate_decision not in decision_set:\n",
    "            performance_samples_enabled = sorted_new_samples_df[sorted_new_samples_df[candidate_decision] == sorted_new_samples_df.iloc[0][candidate_decision]][nfp]\n",
    "            performance_samples_disabled = sorted_new_samples_df[sorted_new_samples_df[candidate_decision] != sorted_new_samples_df.iloc[0][candidate_decision]][nfp]\n",
    "\n",
    "            welchs_ttest_result = ttest_ind(performance_samples_enabled, performance_samples_disabled, equal_var = False)\n",
    "            # If the p-value is less than 0.05, then the difference is statistically significant, and the decision is added to the noteworthy decisions\n",
    "            if welchs_ttest_result.pvalue < 0.05:\n",
    "                noteworthy_decisions.append((candidate_decision, sorted_new_samples_df.iloc[0][candidate_decision]))\n",
    "            else:\n",
    "                continue\n",
    "    # If there are (new) noteworthy decisions, then sample the configuration space again with the new decision set (recursively)\n",
    "    if noteworthy_decisions:\n",
    "        new_measurement_df = pd.concat([measurement_df, samples_df, samples_df]).drop_duplicates(keep=False)\n",
    "        return SRS_sampling(step_sample_size, new_measurement_df, nfp, samples_df, decision_set = decision_set + noteworthy_decisions, sample_set_size = sample_set_size)\n",
    "    else:\n",
    "        # If there are no noteworthy decisions and there are still samples missing to reach the sample set size, then sample the remaining configuration space for the missing samples\n",
    "        if len(samples_df.index) < sample_set_size:\n",
    "            sample_size = sample_set_size - len(samples_df.index)\n",
    "            new_samples_df = get_n_random_samples(measurement_df, sample_size) if len(measurement_df.index) > sample_size else measurement_df\n",
    "            samples_df = pd.concat([samples_df, new_samples_df])\n",
    "        return samples_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 7z...\n",
      "Processing BerkeleyDBC...\n",
      "Processing BerkeleyDBC...\n",
      "Processing Dune...\n",
      "Processing Dune...\n",
      "Processing Hipacc...\n",
      "Processing Hipacc...\n",
      "Processing JavaGC...\n",
      "Processing JavaGC...\n",
      "Processing LLVM...\n",
      "Processing LLVM...\n",
      "Processing Polly...\n",
      "Processing Polly...\n"
     ]
    }
   ],
   "source": [
    "# configurable system names\n",
    "root_path = \"./data/sampling/\"\n",
    "\n",
    "cs_information = [\n",
    "    (\"7z\", \"Performance\", [\"Variable Features\", \"Size\"], \"7z (Performance)\", 600),\n",
    "    (\"BerkeleyDBC\", \"Performance\", [], \"Berkeley DB (Performance)\", 97),\n",
    "    (\"Dune\", \"Performance\", [\"Variable Features\"], \"Dune (Performance)\", 265),\n",
    "    (\"Hipacc\", \"Performance\", [\"Variable Features\"], \"Hipacc (Performance)\", 843),\n",
    "    (\"JavaGC\", \"Performance\", [\"Variable Features\"], \"Java GC (Performance)\", 468),\n",
    "    (\"LLVM\", \"Performance\", [\"MainMemory\"], \"LLVM (Performance)\", 56),\n",
    "    (\"Polly\", \"Performance\", [\"Variable Features\", \"ElapsedTime\"], \"Polly (Performance)\", 345),\n",
    "]\n",
    "\n",
    "# Required size of the sample set\n",
    "sample_set_size = 300\n",
    "# Number of different sample sets to draw per configurable system\n",
    "number_repetitions = 100\n",
    "\n",
    "for cs, nfp_type, exclusion_list, diagram_title, t2_size in cs_information:\n",
    "    print(\"Processing \" + cs + \"...\")\n",
    "    # read the data\n",
    "    measurement_df = pd.read_csv(root_path + cs + \"/measurements.csv\")\n",
    "    # remove the excluded columns\n",
    "    measurement_df = measurement_df.drop(columns = exclusion_list)\n",
    "\n",
    "    for i in range(1, number_repetitions + 1):\n",
    "        # Set the random seed for sampling with pandas sample method (to ensure reproducibility)\n",
    "        np.random.seed(i)\n",
    "\n",
    "        # sample the configuration space using the SRS algorithm for #sample_set_size configurations\n",
    "        sample_set = SRS_sampling(25, measurement_df, nfp_type, sample_set_size = sample_set_size)\n",
    "        # sample the configuration space using the SRS algorithm for #t2_size configurations\n",
    "        sample_set_t2 = SRS_sampling(25, measurement_df, nfp_type, sample_set_size = t2_size)\n",
    "\n",
    "        # remove the NFP columns for writing the sample set to a csv file\n",
    "        sample_set = sample_set.drop(columns = nfp_type)\n",
    "        sample_set_t2 = sample_set_t2.drop(columns = nfp_type)\n",
    "\n",
    "        # write the sample sets to a csv file\n",
    "        sample_set.to_csv(root_path + cs + \"/\" + cs + \"_\" + str(i) + \"/sampledConfigurations_SRSSampling_sampleSize_\" + str(sample_set_size) + \".csv\", index = False)\n",
    "        sample_set_t2.to_csv(root_path + cs + \"/\" + cs + \"_\" + str(i) + \"/sampledConfigurations_SRSSampling_t2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-env (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
